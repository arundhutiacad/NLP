{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUWTvIyEovQn",
        "outputId": "036fbb42-344a-486f-a6ea-4e1e75bbe195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 23.277075052261353\n",
            "Epoch 2, Loss: 22.667892694473267\n",
            "Epoch 3, Loss: 22.502378225326538\n",
            "Epoch 4, Loss: 21.587493658065796\n",
            "Epoch 5, Loss: 20.702043294906616\n",
            "['wait', 'act', 'make', 'iron', 'not']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 100\n",
        "context_size = 2  # Number of context words to use\n",
        "num_negative_samples = 5  # Number of negative samples per positive sample\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "# Example corpus\n",
        "corpus = [\n",
        "    \"we are what we repeatedly do excellence then is not an act but a habit\",\n",
        "    \"the only way to do great work is to love what you do\",\n",
        "    \"if you can dream it you can do it\",\n",
        "    \"do not wait to strike till the iron is hot but make it hot by striking\",\n",
        "    \"whether you think you can or you think you cannot you are right\",\n",
        "]\n",
        "\n",
        "# Preprocess the corpus\n",
        "def preprocess_corpus(corpus):\n",
        "    words = [word for sentence in corpus for word in sentence.split()]\n",
        "    vocab = set(words)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    return words, word_to_idx, idx_to_word\n",
        "\n",
        "words, word_to_idx, idx_to_word = preprocess_corpus(corpus)\n",
        "\n",
        "# Generate training data\n",
        "def generate_training_data(words, word_to_idx, context_size):\n",
        "    data = []\n",
        "    for i in range(context_size, len(words) - context_size):\n",
        "        target_word = word_to_idx[words[i]]\n",
        "        context_words = [word_to_idx[words[i - j - 1]] for j in range(context_size)]\n",
        "        context_words += [word_to_idx[words[i + j + 1]] for j in range(context_size)]\n",
        "        for context_word in context_words:\n",
        "            data.append((target_word, context_word))\n",
        "    return data\n",
        "\n",
        "training_data = generate_training_data(words, word_to_idx, context_size)\n",
        "\n",
        "# Custom Dataset class\n",
        "class Word2VecDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = Word2VecDataset(training_data)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Negative Sampling\n",
        "def get_negative_samples(target, num_negative_samples, vocab_size):\n",
        "    neg_samples = []\n",
        "    while len(neg_samples) < num_negative_samples:\n",
        "        neg_sample = np.random.randint(0, vocab_size)\n",
        "        if neg_sample != target:\n",
        "            neg_samples.append(neg_sample)\n",
        "    return neg_samples\n",
        "\n",
        "# Skip-gram Model with Negative Sampling\n",
        "class SkipGramNegSampling(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(SkipGramNegSampling, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.log_sigmoid = nn.LogSigmoid()\n",
        "\n",
        "    def forward(self, target, context, negative_samples):\n",
        "        target_embedding = self.embeddings(target)\n",
        "        context_embedding = self.context_embeddings(context)\n",
        "        negative_embeddings = self.context_embeddings(negative_samples)\n",
        "\n",
        "        positive_score = self.log_sigmoid(torch.sum(target_embedding * context_embedding, dim=1))\n",
        "        negative_score = self.log_sigmoid(-torch.bmm(negative_embeddings, target_embedding.unsqueeze(2)).squeeze(2)).sum(1)\n",
        "\n",
        "        loss = - (positive_score + negative_score).mean()\n",
        "        return loss\n",
        "\n",
        "# Training the model\n",
        "vocab_size = len(word_to_idx)\n",
        "model = SkipGramNegSampling(vocab_size, embedding_dim)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for target, context in dataloader:\n",
        "        target = target.long()\n",
        "        context = context.long()\n",
        "        negative_samples = torch.LongTensor([get_negative_samples(t.item(), num_negative_samples, vocab_size) for t in target])\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = model(target, context, negative_samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Getting the word embeddings\n",
        "embeddings = model.embeddings.weight.detach().numpy()\n",
        "\n",
        "# Function to get similar words\n",
        "def get_similar_words(word, top_n=5):\n",
        "    idx = word_to_idx[word]\n",
        "    word_embedding = embeddings[idx]\n",
        "    similarities = np.dot(embeddings, word_embedding)\n",
        "    closest_idxs = (-similarities).argsort()[1:top_n+1]\n",
        "    return [idx_to_word[idx] for idx in closest_idxs]\n",
        "\n",
        "# Example usage\n",
        "print(get_similar_words(\"do\"))\n"
      ]
    }
  ]
}