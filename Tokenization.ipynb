{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"Tokenization is crucial for NLP.\"\n",
        "word_tokens = word_tokenize(text)\n",
        "print(\"Word Tokens:\", word_tokens)\n"
      ],
      "metadata": {
        "id": "-xh8Q-RpryX4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f32167-b84c-4fde-868f-1b6801316817"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Tokenization', 'is', 'crucial', 'for', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.trainers import BpeTrainer\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "tokenizer.pre_tokenizer = Whitespace()\n",
        "\n",
        "training_data = [\"unhappiness\", \"tokenization\"]\n",
        "trainer = BpeTrainer(special_tokens=[\"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
        "tokenizer.train_from_iterator(training_data, trainer)\n",
        "\n",
        "output = tokenizer.encode(\"unhappiness\")\n",
        "print(\"Subword Tokens:\", output.tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtlHqWFHEYMg",
        "outputId": "bca3d3c5-1280-48a2-fee6-863ba29b4e82"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subword Tokens: ['unhappiness']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenization\"\n",
        "character_tokens = list(text)\n",
        "print(\"Character Tokens:\", character_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3z2JW2SHGU7",
        "outputId": "4066f1bc-0d2f-44d4-e31d-54df57ab9723"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokens: ['T', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Tokenization is crucial for NLP.\"\n",
        "word_tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "print(\"Word Tokens:\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AAeO-ud2H8rr",
        "outputId": "75ba8be0-5411-48c0-9f93-32347f6c3ca3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Tokenization', 'is', 'crucial', 'for', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "\n",
        "text = \"我喜欢自然语言处理\"\n",
        "word_tokens = jieba.lcut(text)\n",
        "print(\"Word Tokens:\", word_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqUjgXVzI7ZQ",
        "outputId": "3509d267-b0d2-4f65-a9c4-b33a9421f051"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Dumping model to file cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Dumping model to file cache /tmp/jieba.cache\n",
            "Loading model cost 1.718 seconds.\n",
            "DEBUG:jieba:Loading model cost 1.718 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['我', '喜欢', '自然语言', '处理']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "text = \"Tokenization is crucial for NLP.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "word_tokens = [token.text for token in doc]\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "\n",
        "sentence_tokens = [sent.text for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1OcVqKJMXht",
        "outputId": "9cda1579-7504-4404-ae0c-a1088a888c96"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Tokenization', 'is', 'crucial', 'for', 'NLP', '.']\n",
            "Sentence Tokens: ['Tokenization is crucial for NLP.']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}